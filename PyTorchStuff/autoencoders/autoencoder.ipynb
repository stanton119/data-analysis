{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Image compression - part 2. - Autoencoders\n"," In this post I will be looking at building an autoencoder to compress the MNIST dataset.\n"," See part 1. [here](https://github.com/stanton119/data-analysis/blob/master/PyTorchStuff/autoencoders/pca.md).\n","\n"," Autoencoders build a network to encode the original images into a latent space and then build a decoder\n"," to reproduce back the same image.\n"," By having a latent space representation that is small we force the network to compress the information.\n"," The latent space is similar to the concept of components within PCA, but in this case the encoder and decoder\n"," can be nonlinear.\n"," The PCA weights that form components can be seen as a subset of the possible solutions for the autoencoder.\n"," As such I expect the autoencoder to do at least as good as PCA on compressing the images.\n","\n"," First let's download the required dataset. This time we download the test set as well to inform us on overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","\n","plt.style.use(\"seaborn-whitegrid\")\n","\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","mnist_train_data = torchvision.datasets.MNIST(\n","    Path() / \"data\", train=True, download=True, transform=transform\n",")\n","mnist_train = torch.utils.data.DataLoader(mnist_train_data, batch_size=64)\n","\n","mnist_test_data = torchvision.datasets.MNIST(\n","    Path() / \"data\", train=False, download=True, transform=transform\n",")\n","mnist_test = torch.utils.data.DataLoader(mnist_test_data, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":[" ## Dense Autoencoder\n"," Our first autoencoder will be based on dense layers.\n"," I may follow up with a comparison to convolution layers which are typically used in image based problems.\n","\n"," This is a pretty standard setup where the output size of each layer of the encoder shrinks toward the latent dimension.\n"," Images are reshaped into a vector and processed through normal dense layers.\n"," The decode effectively inverses the operations of the encoder.\n"," It uses a sigmoid activation at the end to ensure the correct pixel value range is observed.\n"," To optimise, we are minimising the reconstruction MSE."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pytorch_lightning as pl\n","\n","\n","class AutoEncoderDense(pl.LightningModule):\n","    def __init__(self, n_inputs: int = 1, n_latent: int = 5):\n","        super().__init__()\n","        self.train_log = []\n","        self.n_latent = n_latent\n","\n","        self.encoder = torch.nn.Sequential(\n","            torch.nn.Linear(28 * 28, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, n_latent),\n","        )\n","\n","        self.decoder = torch.nn.Sequential(\n","            torch.nn.Linear(n_latent, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 28 * 28),\n","            torch.nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        x = x.reshape(-1, 28 * 28)\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded.reshape(-1, 1, 28, 28)\n","\n","    def configure_optimizers(self, learning_rate=1e-3):\n","        optimizer = torch.optim.Adam(\n","            self.parameters(),\n","            lr=learning_rate,\n","        )\n","        return optimizer\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        x_hat = self(x)\n","        loss = torch.nn.MSELoss()(x_hat, x)\n","\n","        self.log(\"loss\", loss)\n","        self.train_log.append(loss.detach().numpy())\n","        return loss"]},{"cell_type":"markdown","metadata":{},"source":[" Compared to our PCA example, the number of parameters we are tuning here is significantly larger.\n"," In fact larger than our training set of 60k examples:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_dense = AutoEncoderDense(n_latent=10)\n","print(model_dense.summarize())"]},{"cell_type":"markdown","metadata":{},"source":[" We will examine this is more detail later.\n","\n"," ## Training\n"," We will now create several dense networks with different latent space sizes.\n"," We save the networks each time so that we can recall them later for predictions.\n"," Plotting the training MSE shows if the model has converged successfully."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["latent_space_dim = [3, 5, 10, 20, 30, 50]\n","model_path = Path() / \"models\"\n","model_path.mkdir(exist_ok=True)\n","\n","for n_latent in latent_space_dim:\n","    print(f\"training: {n_latent}\")\n","    model_dense = AutoEncoderDense(n_latent=n_latent)\n","    trainer = pl.Trainer(max_epochs=10)\n","    trainer.fit(model_dense, mnist_train)\n","    torch.save(model_dense, model_path / f\"dense_{n_latent}.pt\")\n","\n","    fig, ax = plt.subplots(figsize=(10, 6))\n","    ax.plot(model_dense.train_log)\n","    ax.set_title(f\"Training error: {n_latent}\")\n","    ax.set_xlabel(\"Batches\")\n","    ax.set_ylabel(\"MSE\")"]},{"cell_type":"markdown","metadata":{},"source":[" ## Results\n"," We need to get the MSE of all images so we can see how the latent space affects reconstruction error.\n"," For this we reload each network and predict all the training images."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# use whole training dataset\n","dataloader = torch.utils.data.DataLoader(\n","    dataset=mnist_train_data, batch_size=len(mnist_train_data)\n",")\n","images_all, labels_all = next(iter(dataloader))\n","\n","# dense model error\n","mse_train_dense = []\n","for n_latent in latent_space_dim:\n","    print(f\"mse: {n_latent}\")\n","    model_dense = torch.load(model_path / f\"dense_{n_latent}.pt\")\n","    images_all_hat = model_dense(images_all)\n","    _loss = torch.nn.MSELoss()(images_all_hat, images_all)\n","    mse_train_dense.append(_loss.detach().numpy())"]},{"cell_type":"markdown","metadata":{},"source":[" To examine the results of the networks we will compare against PCA as a baseline.\n"," Here we fit a PCA model as previously shown.\n"," Then we reconstruct the images and measure the MSE at each latent space size."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import sklearn.decomposition\n","import sklearn.metrics\n","\n","# convert images to 1D vectors\n","images_flat = images_all[:, 0].reshape(-1, 784).numpy()\n","images_flat.shape\n","\n","print(f\"training components: {latent_space_dim[-1]}\")\n","pca = sklearn.decomposition.PCA(n_components=latent_space_dim[-1])\n","images_flat_hat = pca.inverse_transform(pca.fit_transform(images_flat))\n","\n","\n","def transform_truncated(pca, X, n_components):\n","    X = pca._validate_data(X, dtype=[np.float64, np.float32], reset=False)\n","    if pca.mean_ is not None:\n","        X = X - pca.mean_\n","    X_transformed = np.dot(X, pca.components_[:n_components, :].T)\n","    if pca.whiten:\n","        X_transformed /= np.sqrt(pca.explained_variance_)\n","    return X_transformed\n","\n","\n","def inv_transform(pca, X, n_components):\n","    return np.dot(X, pca.components_[:n_components, :]) + pca.mean_\n","\n","\n","def inv_forward_transform(pca, X, n_components):\n","    return inv_transform(\n","        pca, transform_truncated(pca, X, n_components), n_components\n","    )\n","\n","\n","# get pca mse\n","mse_train_pca = []\n","for n_latent in latent_space_dim:\n","    print(f\"mse: {n_latent}\")\n","    images_flat_hat = inv_forward_transform(\n","        pca, X=images_flat, n_components=n_latent\n","    )\n","    _loss = sklearn.metrics.mean_squared_error(images_flat_hat, images_flat)\n","    mse_train_pca.append(_loss)"]},{"cell_type":"markdown","metadata":{},"source":[" Now let's plot the two approaches side by side:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# reconstruction mse\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(latent_space_dim, mse_train_dense, label=\"dense\")\n","ax.plot(latent_space_dim, mse_train_pca, label=\"pca\")\n","ax.set_title(\"Reconstruction error\")\n","ax.set_xlabel(\"Latent space size\")\n","ax.set_ylabel(\"MSE\")\n","fig.legend()"]},{"cell_type":"markdown","metadata":{},"source":[" We can see that the dense autoencoder does do better generally.\n"," Particularly so at small latent space sizes.\n"," Once the latent space gets much larger PCA becomes comparible.\n"," With a latent space of 50, in the autoencoder this is greater than the output\n"," size of the preceeding layer, hence we dont expect any improvement here.\n","\n"," ## Test set\n"," However as noted prior, there are more parameters than images, so we could easily be overfitting here.\n"," To confirm we can check the reconstruction error on the unseen test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run same analysis on test set to check for overfitting\n","\n","# use whole training dataset\n","dataloader = torch.utils.data.DataLoader(\n","    dataset=mnist_test_data, batch_size=len(mnist_test_data)\n",")\n","images_all, labels_all = next(iter(dataloader))\n","images_flat = images_all[:, 0].reshape(-1, 784).numpy()\n","\n","# autoencoder\n","mse_test_dense = []\n","for n_latent in latent_space_dim:\n","    print(f\"mse: {n_latent}\")\n","    model_dense = torch.load(model_path / f\"dense_{n_latent}.pt\")\n","    images_all_hat = model_dense(images_all)\n","    _loss = torch.nn.MSELoss()(images_all_hat, images_all)\n","    mse_test_dense.append(_loss.detach().numpy())\n","\n","# pca\n","mse_test_pca = []\n","for n_latent in latent_space_dim:\n","    print(f\"mse: {n_latent}\")\n","    images_flat_hat = inv_forward_transform(\n","        pca, X=images_flat, n_components=n_latent\n","    )\n","    _loss = sklearn.metrics.mean_squared_error(images_flat_hat, images_flat)\n","    mse_test_pca.append(_loss)\n","\n","\n","# reconstruction mse\n","fig, ax = plt.subplots(figsize=(10, 6))\n","ax.plot(latent_space_dim, mse_test_dense, label=\"dense\")\n","ax.plot(latent_space_dim, mse_test_pca, label=\"pca\")\n","ax.set_title(\"Reconstruction error\")\n","ax.set_xlabel(\"Latent space size\")\n","ax.set_ylabel(\"MSE\")\n","fig.legend()"]},{"cell_type":"markdown","metadata":{},"source":[" We obtain very similar results to before.\n"," This gives us a good indication we are not overfitting.\n"," Therefore the autoencoders should generalise to unseen images fine.\n"," For more confidence it would be nice to apply cross validation and get multiple instances of the model and results.\n"," I'll skip this for now in the interests of time.\n","\n"," ## Results - images\n"," We have an improvement in MSE but it's good to check the actual reconstructed images to confirm with our eyes.\n","\n"," First for PCA - top row are the originals, subsequent rows are increasing latent space size."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(20, 20), ncols=6, nrows=5)\n","\n","for row, n_latent in enumerate(latent_space_dim[:4]):\n","    images_hat = inv_forward_transform(\n","        pca, X=images_flat, n_components=n_latent\n","    ).reshape(-1, 28, 28)\n","\n","    for col in range(6):\n","        ax[0, col].imshow(images_all[col, 0])\n","        ax[0, col].set_title(str(labels_all[col].numpy()))\n","\n","        ax[row + 1, col].imshow(images_hat[col])\n","        ax[row + 1, col].set_title(str(labels_all[col].numpy()))"]},{"cell_type":"markdown","metadata":{},"source":[" The same for the autoencoder:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(20, 20), ncols=6, nrows=5)\n","\n","for row, n_latent in enumerate(latent_space_dim[:4]):\n","    model_dense = torch.load(model_path / f\"dense_{n_latent}.pt\")\n","    images_hat = model_dense(images_all).detach()\n","    \n","    for col in range(6):\n","        ax[0, col].imshow(images_all[col, 0])\n","        ax[0, col].set_title(str(labels_all[col].numpy()))\n","\n","        ax[row + 1, col].imshow(images_hat[col,0])\n","        ax[row + 1, col].set_title(str(labels_all[col].numpy()))"]},{"cell_type":"markdown","metadata":{},"source":[" We can see that the autoencoder is much clearer at small latent spaces.\n"," Even at only 3, the images are pretty decent.\n","\n"," Similar to PCA, some digits look worse than others.\n"," We can plot the MSE against the digit to see which are hard to construct:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MSE against label - PCA benchmark\n","images_flat_hat = inv_forward_transform(\n","    pca, X=images_flat, n_components=latent_space_dim[2]\n",")\n","\n","loss_label_pca = []\n","for label in range(0, 10):\n","    filt = labels_all == label\n","    _loss = sklearn.metrics.mean_squared_error(\n","        images_flat_hat[filt], images_flat[filt]\n","    )\n","    loss_label_pca.append(_loss)\n","\n","# MSE against label for autoencoder\n","loss_label = []\n","for row, n_latent in enumerate(latent_space_dim):\n","    model_dense = torch.load(model_path / f\"dense_{n_latent}.pt\")\n","    images_all_hat = model_dense(images_all)\n","\n","    _loss_label = []\n","    for label in range(0, 10):\n","        filt = labels_all == label\n","        _loss = torch.nn.MSELoss()(\n","            images_all_hat[filt].detach(), images_all[filt].detach()\n","        ).numpy().flatten()[0]\n","        _loss_label.append(_loss)\n","    loss_label.append(_loss_label)\n","\n","# create plot with pca benchmark\n","df_loss = pd.DataFrame(\n","    loss_label, index=latent_space_dim, columns=range(0, 10)\n",").transpose()\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","df_loss.plot(ax=ax, legend=False)\n","ax.plot(range(0, 10), loss_label_pca, '--', label=f'pca_{latent_space_dim[2]}')\n","ax.set_title(\"Reconstruction error by digit number\")\n","ax.set_xlabel(\"Digit label\")\n","ax.set_ylabel(\"MSE\")\n","fig.legend()"]},{"cell_type":"markdown","metadata":{},"source":[" The digits the autoencoder struggle with are generally the same as PCA.\n"," We can see the reconstruction error for an autoencoder with 5 latent variables is comparible\n"," to PCA with 10 components.\n"," The autoencoder seems to do better reconstructing '1', '6' and '7'.\n","\n"," There are plenty of hyperparameters to tune with the autoencoder.\n"," For example all the other layer sizes and number of layers.\n"," Indeed we could switch the dense layers out for convolution layers...\n","\n"," Another thought, it should be fairly easy to train a digit classifier on the latent representation of the images."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}