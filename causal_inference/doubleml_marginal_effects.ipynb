{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoubleML with binary variables and marginal effects\n",
    "\n",
    "When using DoubleML, what do we do with binary outcomes or binary treatments? It seems like we just plough ahead and we the same models we would have if we used continuous outcomes.\n",
    "\n",
    "Here we explore the effects of using appropriate models and 'less' appropriate models to see if there's an actual difference.\n",
    "\n",
    "Ref:\n",
    "https://github.com/py-why/EconML/issues/204"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy data\n",
    "\n",
    "We create a data generating function to produce some dummy features with confounding and generate random outcomes from a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def print_results(array):\n",
    "    print([f\"{_x:.3f}\" for _x in array])\n",
    "\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "\n",
    "def no_confound(x: np.array):\n",
    "    return np.zeros(x.shape[0])\n",
    "\n",
    "\n",
    "def linear_confound(x: np.array, a: float = 1.0, b: float = 0.5):\n",
    "    return a * x[:, 0] + b\n",
    "\n",
    "\n",
    "def generate_treatment_data(\n",
    "    n_samples: int = 1000,\n",
    "    n_features: int = 4,\n",
    "    treatment_binary: bool = False,\n",
    "    seed: int = None,\n",
    "    confounding_fcn: callable = None,\n",
    "    treatment_noise: float = 0.1,\n",
    "):\n",
    "    if confounding_fcn is None:\n",
    "        confounding_fcn = no_confound\n",
    "\n",
    "    rand = np.random.default_rng(seed)\n",
    "\n",
    "    # generate random features\n",
    "    x = rand.normal(\n",
    "        loc=rand.normal(size=n_features),\n",
    "        scale=rand.exponential(size=n_features),\n",
    "        size=(n_samples, n_features),\n",
    "    )\n",
    "\n",
    "    t_x = confounding_fcn(x)\n",
    "    if treatment_binary:\n",
    "        t = rand.binomial(n=1, p=inv_logit(t_x), size=n_samples)\n",
    "    else:\n",
    "        t = treatment_noise * rand.normal(size=n_samples) + t_x\n",
    "\n",
    "    x = np.concatenate([t[:, np.newaxis], x], axis=1)\n",
    "\n",
    "    t_col = \"t\"\n",
    "    x_cols = [f\"x_{idx+1}\" for idx in range(n_features)]\n",
    "\n",
    "    return pd.DataFrame(data=x, columns=[t_col] + x_cols), t_col, x_cols\n",
    "\n",
    "\n",
    "def generate_outcome_data(\n",
    "    x: pd.DataFrame,\n",
    "    outcome_binary: bool = False,\n",
    "    outcome_noise: float = 0.1,\n",
    "    seed: int = None,\n",
    "    bias: float = None,\n",
    "    weights: np.array = None,\n",
    "):\n",
    "    rand = np.random.default_rng(seed)\n",
    "\n",
    "    n_samples, n_features = x.shape\n",
    "    if bias is None:\n",
    "        bias = rand.normal()\n",
    "    if weights is None:\n",
    "        weights = rand.normal(size=(n_features, 1))\n",
    "    y = bias + np.dot(x, weights) + outcome_noise * rand.normal()\n",
    "\n",
    "    if outcome_binary:\n",
    "        y_avg = inv_logit(y)\n",
    "        y = rand.binomial(n=1, p=y_avg, size=(n_samples, 1))\n",
    "    else:\n",
    "        y_avg = None\n",
    "\n",
    "    return y, bias, weights, y_avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear case\n",
    "\n",
    "We start with the linear case.\n",
    "\n",
    "We create data from a linear model where a linear regression model would be ideal.\n",
    "\n",
    "We:\n",
    "1. generate the data\n",
    "2. fit a linear regression model with all the features and treatment\n",
    "3. fit a linear regression model with only the treatment, ignoring the confounding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est weights, all features:\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est weights, missing confounders, biased results:\n",
      "['0.479']\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# generate data\n",
    "x_df, t_col, x_cols = generate_treatment_data(\n",
    "    treatment_binary=False, confounding_fcn=linear_confound, seed=0\n",
    ")\n",
    "y, bias, weights, _ = generate_outcome_data(x=x_df, outcome_binary=False, seed=0)\n",
    "\n",
    "print(\"True weights\")\n",
    "print_results(weights.flatten())\n",
    "\n",
    "\n",
    "# fit models\n",
    "linear_model = sm.OLS(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, all features:\")\n",
    "print_results(np.array(linear_model.params)[1:])\n",
    "\n",
    "linear_model = sm.OLS(y, sm.add_constant(x_df[t_col])).fit()\n",
    "print(\"Est weights, missing confounders, biased results:\")\n",
    "print_results(np.array(linear_model.params)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression works as expected and when we don't include the confounder features we get a biased estimate for the treatment uplift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate marginal effects\n",
    "The marginal effect represents the change in `y` given a unit change in the treatment `t` (or the features `x` as well).\n",
    "\n",
    "We calculate this by estimating derivatives in `y` for each feature to represent the average marginal effects.\n",
    "For the linear data model, these marginal effects are the same as the data generating weights.\n",
    "When we estimate them, the results are, as expected, asymptotically equal to the estimate linear regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n"
     ]
    }
   ],
   "source": [
    "def get_marginal_effects(x, d_x: float = 1.0, model_fcn=None):\n",
    "    d_y = []\n",
    "    for col in x.columns:\n",
    "        _x = x.copy()\n",
    "        _x[col] = _x[col] + d_x\n",
    "        d_y.append((model_fcn(_x) - model_fcn(x)).mean() / d_x)\n",
    "\n",
    "    return d_y\n",
    "\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(\n",
    "    get_marginal_effects(\n",
    "        x=x_df,\n",
    "        model_fcn=lambda x: generate_outcome_data(\n",
    "            x, outcome_binary=False, bias=bias, weights=weights, seed=0\n",
    "        )[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double ML\n",
    "\n",
    "Assume all features are confounders.\n",
    "Assume no features for CATE.\n",
    "\n",
    "We can use GBMs for the models for `y` and `t` without creating bias.\n",
    "We get similar results if we use linear regression for these.\n",
    "\n",
    "As the data is generated from a linear model, linear regression is the ideal model to use, so we get slightly worse results with GBMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est marginal effect - linear models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>        <th>zstat</th>        <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.132</td>       <td>0.0</td>     <td>-1232558224036509.5</td>   <td>0.0</td>     <td>-0.132</td>        <td>-0.132</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.132</td>          <td>-0.132</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>0.0</td>         <td>-0.132</td>         <td>-0.132</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x15c8e7d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est marginal effect - GBMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th> <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.092</td>      <td>0.033</td>    <td>-2.816</td>  <td>0.005</td>    <td>-0.156</td>        <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.092</td>          <td>-0.092</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>0.033</td>        <td>-0.156</td>         <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x1099a97b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import econml.dml\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "est = econml.dml.LinearDML(\n",
    "    model_t=sklearn.linear_model.LinearRegression(),\n",
    "    model_y=sklearn.linear_model.LinearRegression(),\n",
    "    random_state=0,\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"True weights\")\n",
    "print_results(weights.flatten())\n",
    "\n",
    "print(\"Est marginal effect - linear models\")\n",
    "display(est.const_marginal_ate_inference())\n",
    "\n",
    "\n",
    "est = econml.dml.LinearDML(\n",
    "    model_t=sklearn.ensemble.GradientBoostingRegressor(random_state=0),\n",
    "    model_y=sklearn.ensemble.GradientBoostingRegressor(random_state=0),\n",
    "    random_state=0,\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"Est marginal effect - GBMs\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have agreement for the treatment effect over linear regression, doubleML and this matches the average marginal effect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary outcome case\n",
    "\n",
    "In this case the linear model coefficients are not the same as the average marginal effect.\n",
    "\n",
    "Here, the marginal effect is the average change in probability of observing a positive outcome, also known as percentage basis points.\n",
    "\n",
    "Here we see that the linear regression coefficients do not match the data weights, as expected.\n",
    "The logistic regression model is the appropriate model to use and we recover the data weights without bias.\n",
    "\n",
    "Side note: As the binary outcome introduces a lot of noise we increase the number of samples to get reasonable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est weights, linear regression:\n",
      "['-0.024', '0.143', '0.024', '-0.124', '0.084']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657852\n",
      "         Iterations 5\n",
      "Est weights, logistic regression:\n",
      "['-0.104', '0.613', '0.103', '-0.533', '0.360']\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "x_df, t_col, x_cols = generate_treatment_data(\n",
    "    treatment_binary=False, confounding_fcn=linear_confound, n_samples=int(1e6), seed=0\n",
    ")\n",
    "y, bias, weights, y_avg = generate_outcome_data(x=x_df, outcome_binary=True, seed=0)\n",
    "\n",
    "print(\"True weights\")\n",
    "print_results(weights.flatten())\n",
    "\n",
    "# fit models\n",
    "linear_model = sm.OLS(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, linear regression:\")\n",
    "print_results(np.array(linear_model.params)[1:])\n",
    "\n",
    "logit_model = sm.Logit(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, logistic regression:\")\n",
    "print_results(np.array(logit_model.params)[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we find the average marginal effect from the logistic model we get the linear regression coefficients.\n",
    "\n",
    "We use the average `y` to calculate the true marginal effects as it reduces the noise a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.148', '0.024', '-0.119', '0.084']\n",
      "Average marginal effect, logistic regression:\n",
      "['-0.024', '0.143', '0.024', '-0.124', '0.084']\n",
      "Est weights, linear regression:\n",
      "['-0.024', '0.143', '0.024', '-0.124', '0.084']\n"
     ]
    }
   ],
   "source": [
    "print(\"True est. marginal effects:\")\n",
    "marginal_effects = get_marginal_effects(\n",
    "    x=x_df,\n",
    "    model_fcn=lambda x: generate_outcome_data(\n",
    "        x, outcome_binary=True, bias=bias, weights=weights, seed=0\n",
    "    )[-1],\n",
    ")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "\n",
    "print(\"Average marginal effect, logistic regression:\")\n",
    "print_results(logit_model.get_margeff().margeff)\n",
    "\n",
    "print(\"Est weights, linear regression:\")\n",
    "print_results(np.array(linear_model.params)[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we are interested in the average marginal effects we don't need to use logistic regression.\n",
    "\n",
    "This opens the door to use doubleML with regressors to estimate the average marginal effect even if we have binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.148', '0.024', '-0.119', '0.084']\n",
      "Est marginal effect\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th> <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.024</td>      <td>0.005</td>    <td>-5.018</td>   <td>0.0</td>     <td>-0.033</td>        <td>-0.015</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.024</td>          <td>-0.024</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>0.005</td>        <td>-0.033</td>         <td>-0.015</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x1514cc130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "    est = econml.dml.LinearDML(\n",
    "        model_t=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "        model_y=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "        random_state=0,\n",
    "    )\n",
    "else:\n",
    "    est = econml.dml.LinearDML(\n",
    "        model_t=sklearn.linear_model.LinearRegression(),\n",
    "        model_y=sklearn.linear_model.LinearRegression(),\n",
    "        random_state=0,\n",
    "    )\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "print(\"Est marginal effect\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary treatment case\n",
    "\n",
    "If we have a binary treatment the final model should be a linear regression.\n",
    "\n",
    "Here the marginal effects are equal to the data weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est weights, linear regression:\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "True est. marginal effects:\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "x_df, t_col, x_cols = generate_treatment_data(\n",
    "    treatment_binary=True, confounding_fcn=linear_confound, n_samples=int(1e6), seed=0\n",
    ")\n",
    "y, bias, weights, y_avg = generate_outcome_data(x=x_df, outcome_binary=False, seed=0)\n",
    "\n",
    "print(\"True weights\")\n",
    "print_results(weights.flatten())\n",
    "\n",
    "\n",
    "# fit models\n",
    "linear_model = sm.OLS(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, linear regression:\")\n",
    "print_results(np.array(linear_model.params)[1:])\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "marginal_effects = get_marginal_effects(\n",
    "    x=x_df,\n",
    "    model_fcn=lambda x: generate_outcome_data(\n",
    "        x, outcome_binary=False, bias=bias, weights=weights, seed=0\n",
    "    )[0],\n",
    ")\n",
    "print_results(marginal_effects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that using regression based double ML returns the same marginal effect back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "Est marginal effect\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>   <th>zstat</th>   <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.132</td>       <td>0.0</td>     <td>-1795.605</td>   <td>0.0</td>     <td>-0.132</td>        <td>-0.132</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.132</td>          <td>-0.132</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>0.0</td>         <td>-0.132</td>         <td>-0.132</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x1099ab6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "    est = econml.dml.LinearDML(\n",
    "        model_t=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "        model_y=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "        random_state=0,\n",
    "    )\n",
    "else:\n",
    "    est = econml.dml.LinearDML(\n",
    "        model_t=sklearn.linear_model.LinearRegression(),\n",
    "        model_y=sklearn.linear_model.LinearRegression(),\n",
    "        random_state=0,\n",
    "    )\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "print(\"Est marginal effect\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary outcome and treatment case\n",
    "\n",
    "We see that the marginal effect from a logistic model, linear regression coefficient and the data marginal effect match well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights\n",
      "['-0.132', '0.640', '0.105', '-0.536', '0.362']\n",
      "True est. marginal effects:\n",
      "['-0.030', '0.147', '0.024', '-0.118', '0.083']\n",
      "Est weights, linear regression:\n",
      "['-0.030', '0.148', '0.024', '-0.123', '0.083']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654005\n",
      "         Iterations 5\n",
      "Est weights, logistic regression:\n",
      "['-0.130', '0.641', '0.104', '-0.533', '0.358']\n",
      "Average marginal effect, logistic regression:\n",
      "['-0.030', '0.148', '0.024', '-0.123', '0.083']\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "x_df, t_col, x_cols = generate_treatment_data(\n",
    "    treatment_binary=True, confounding_fcn=linear_confound, n_samples=int(1e6), seed=0\n",
    ")\n",
    "y, bias, weights, y_avg = generate_outcome_data(x=x_df, outcome_binary=True, seed=0)\n",
    "\n",
    "print(\"True weights\")\n",
    "print_results(weights.flatten())\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "marginal_effects = get_marginal_effects(\n",
    "    x=x_df,\n",
    "    model_fcn=lambda x: generate_outcome_data(\n",
    "        x, outcome_binary=True, bias=bias, weights=weights, seed=0\n",
    "    )[-1],\n",
    ")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "\n",
    "# fit models\n",
    "linear_model = sm.OLS(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, linear regression:\")\n",
    "print_results(np.array(linear_model.params)[1:])\n",
    "\n",
    "logit_model = sm.Logit(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, logistic regression:\")\n",
    "print_results(np.array(logit_model.params)[1:])\n",
    "\n",
    "print(\"Average marginal effect, logistic regression:\")\n",
    "print_results(logit_model.get_margeff().margeff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that using regression based double ML returns the same marginal effect back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.147', '0.024', '-0.118', '0.083']\n",
      "Est marginal effect\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th>  <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>-0.03</td>      <td>0.001</td>    <td>-28.912</td>   <td>0.0</td>     <td>-0.032</td>        <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>         <td>-0.03</td>           <td>-0.03</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>0.001</td>        <td>-0.032</td>         <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x15f843e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = econml.dml.LinearDML(\n",
    "    model_t=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "    model_y=sklearn.ensemble.HistGradientBoostingRegressor(random_state=0),\n",
    "    random_state=0,\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "print(\"Est marginal effect\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the choise of classifier or regressor matter?\n",
    "\n",
    "This should work, but our residuals from each model would be discrete and therefore less informative. We get answers which are close here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.147', '0.024', '-0.118', '0.083']\n",
      "Est marginal effect\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th> <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.009</td>      <td>0.001</td>    <td>-8.054</td>   <td>0.0</td>     <td>-0.011</td>        <td>-0.007</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.009</td>          <td>-0.009</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>0.001</td>        <td>-0.011</td>         <td>-0.007</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x15f8167a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = econml.dml.LinearDML(\n",
    "    model_t=sklearn.ensemble.HistGradientBoostingClassifier(random_state=0),\n",
    "    model_y=sklearn.ensemble.HistGradientBoostingClassifier(random_state=0),\n",
    "    random_state=0,\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col], X=None, W=x_df.drop(columns=t_col))\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "print(\"Est marginal effect\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the use of discrete treatment matter?\n",
    "\n",
    "This calls the `predict_proba` on the `model_t` object. It also apply one hot encoding to the treatment column.\n",
    "We get very similar answers to using regressor GBMs. Therefore we can likely get away with using a single model definition for all binary and continuous model problems.\n",
    "\n",
    "> There was a bug that required the `_set_transformed_treatment_names` function in econml to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.147', '0.024', '-0.118', '0.083']\n",
      "Est marginal effect\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th>  <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>-0.031</td>      <td>0.001</td>    <td>-23.085</td>   <td>0.0</td>     <td>-0.034</td>        <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Distribution of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>std_point</th> <th>pct_point_lower</th> <th>pct_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0.0</td>        <td>-0.031</td>          <td>-0.031</td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Total Variance of Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>stderr_point</th> <th>ci_point_lower</th> <th>ci_point_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>0.001</td>        <td>-0.034</td>         <td>-0.028</td>    \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<econml.inference._inference.PopulationSummaryResults at 0x15f86ba00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = econml.dml.LinearDML(\n",
    "    model_t=sklearn.ensemble.HistGradientBoostingClassifier(random_state=0),\n",
    "    model_y=sklearn.ensemble.HistGradientBoostingClassifier(random_state=0),\n",
    "    random_state=0,\n",
    "    discrete_treatment=True,\n",
    "    categories=[0, 1],\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col].to_numpy(), X=None, W=x_df.drop(columns=t_col).to_numpy())\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "print(\"Est marginal effect\")\n",
    "display(est.const_marginal_ate_inference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar results with S-Learners\n",
    "\n",
    "If we use an S-learner and find the ATE as E{Y|T=do=1} - E{Y|T=do=0} we should observe the same marginal effect as a logistic regression and linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True est. marginal effects:\n",
      "['-0.030', '0.147', '0.024', '-0.118', '0.083']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est marginal effect - linear regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02994736])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est marginal effect - logistic regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.086629])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import econml.metalearners\n",
    "\n",
    "print(\"True est. marginal effects:\")\n",
    "print_results(marginal_effects)\n",
    "\n",
    "est = econml.metalearners.SLearner(\n",
    "    overall_model=sklearn.linear_model.LinearRegression(),\n",
    "    # categories=[0,1]\n",
    ")\n",
    "T = x_df[t_col].to_numpy()\n",
    "X = x_df.drop(columns=t_col).to_numpy()\n",
    "est.fit(Y=y, T=T, X=X)\n",
    "\n",
    "print(\"Est marginal effect - linear regression\")\n",
    "display(est.ate(x_df.drop(columns=t_col).to_numpy()))\n",
    "\n",
    "est = econml.metalearners.SLearner(\n",
    "    overall_model=sklearn.linear_model.LogisticRegression(penalty=None),\n",
    "    # categories=[0,1]\n",
    ")\n",
    "est.fit(Y=y, T=x_df[t_col].to_numpy(), X=x_df.drop(columns=t_col).to_numpy())\n",
    "\n",
    "print(\"Est marginal effect - logistic regression\")\n",
    "display(est.ate(x_df.drop(columns=t_col).to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m X \u001b[39m=\u001b[39m x_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mt_col)\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m      6\u001b[0m T \u001b[39m=\u001b[39m x_df[t_col]\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m----> 7\u001b[0m np\u001b[39m.\u001b[39mconcatenate((X, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39;49msum(T, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), T), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/project_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/miniconda3/envs/project_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# coefs match\n",
    "est.overall_model.coef_\n",
    "# est.overall_model.predict_proba(x_df.drop(columns=t_col).to_numpy())\n",
    "\n",
    "X = x_df.drop(columns=t_col).to_numpy()\n",
    "T = x_df[t_col].to_numpy()\n",
    "np.concatenate((X, 1 - np.sum(T, axis=1).reshape(-1, 1), T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIcAAAAUCAYAAACnFS7nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAABJ0AAASdAHeZh94AAAD/klEQVR4nO2aW4hWVRiGn9+8yMxGUELoQGkqXURTSSXRRGZjl0JCN1kWRtFhKpLATi+v0AlyJKW6UGmaCOpiOlE33gimKVbYnaUdpAPSwcJUNMqmi7X2zJ49+2/m//89q4j/hc03+1vrXe83w7fXtw5TGxwcpI02yjDp3w6gjf8uJpc5bfcDNwDnSzqWNqQ2UsL2ZcDHwEpJm/NttWJZsb0A2A2sktSb8y8DrgE6gYuBacBrkm4eQ/xsYA0h2WYAB4G3AUv6tQpOCo2UnCpheznQH1/vkLSppM9bwJXAXElHM39ZWXkK+A14qeB/DLiXkBzfjzOwOcAnwG2EhFsHfAXcD+y0PaNVTgqNlJwqYfscYANwdIyuTwOzgJ68c0RZsT0PWAxsknS8MMCDwHfAF4QZZOs44nsROBPokbQhp9Mbx3sSuKtFTgqNlJxKYLsGvAwcAt4EVtXrK2m37c+AO20/K+kkjJ45bgdqwBslA2yVtF/SuLY3tmcD3cAB4IXicMAxYLntqc1yUmik5FSMHmARYdYaz7rxdeBcwuQAjE6OxcBJYFcFwS2Kdoukv/INko4AO4DTCLWuWU4KjZScSmD7QuAZ4HlJ28ZJ2xHt9ZljKDliBncCeyvaocyPdl+d9v3RzmuBk0IjJadl2J4MvAp8AzzSAPWjaLsyR37mOAs4hbCargId0R6u0575p7fASaGRklMFngAuAVaUrBvrQtJh4AShtAAjkyNbOU/49iqiFm0jR7SNclJopOT8I2xfTpgt1kra2cQQvwAzs5d8cmRZdmrz4Y1A9mV01Gk/o9CvGU4KjZScppErJ/uAx5scZgrDeTBiK/tjtFXtvT+Ptl5NnRttviY3ykmhkZLTCk7PaZ2wXdZno+2NhIXqA/kG25MIJe7rzJefOQ4CPzG8kGoV2TlIdxTOBzINuIqQpbta4KTQSMlpBb8Dm+s8e2Kf7fG9rOTMJ5S6TzPHUNDx/GIbMNP2Ba1GKulLYAtwHnBPodnAVKA/vzNqlJNCIyXHdp/tQdsraBCSjktaWfYA78Zur0TfqHMshrfUQ4ebxYu3AeBGYAnhJHT4t7GXAkvj66xoF9ruiz//LKl4Cnc38CGw3vZ1wF7gCuBawnT6aEmQjXJSaKTiZB/rnyVjTTS6CWdc7xSDyTAA/ADcUkLuBG6Nz5Lom53zLSsS4tezAOgj/FEeAuYA64GFkg61ykmhkZBzEXAEeL841kTCdgfhw39P0reZv+xWdjXh8u1SSXtoIwlsTyfcg6yV9HBi7fsICdsl6YPMX3Yru45wurYmUWxtBFwN/AH0jtWxStieAqwGBvKJASUzRyR0Eeric+1/9vl/I97D3AT0STqQb/sbOyGRbWLmvzoAAAAASUVORK5CYII=",
      "text/latex": [
       "$\\displaystyle \\left( 1000000, \\  4\\right)$"
      ],
      "text/plain": [
       "(1000000, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df.drop(columns=t_col).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654005\n",
      "         Iterations 5\n",
      "Est weights, logistic regression:\n",
      "['-0.130', '0.641', '0.104', '-0.533', '0.358']\n",
      "Average marginal effect, logistic regression:\n",
      "['-0.030', '0.148', '0.024', '-0.123', '0.083']\n"
     ]
    }
   ],
   "source": [
    "logit_model = sm.Logit(y, sm.add_constant(x_df[[t_col] + x_cols])).fit()\n",
    "print(\"Est weights, logistic regression:\")\n",
    "print_results(np.array(logit_model.params)[1:])\n",
    "\n",
    "print(\"Average marginal effect, logistic regression:\")\n",
    "print_results(logit_model.get_margeff().margeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intercept_page_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
