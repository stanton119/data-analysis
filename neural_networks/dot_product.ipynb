{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the dot/inner product through an MLP\n",
    "\n",
    "Dot products are commonly used in recommender systems to combine user and item embeddings.\n",
    "\n",
    "The dot product of two vectors (A and B) is the sum of the products of their corresponding elements:\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\sum_{i=1}^{n} a_i b_i\n",
    "$$\n",
    "\n",
    "\n",
    "Dense layers in a neural network work on the weighted sum of inputs. They dont directly capture interactions between features. We can concat two vector and push through a dense layer. The output of a dense layer is given as:\n",
    "$$\n",
    "g([\\mathbf{a} , \\mathbf{b}]) = \\sigma(\\mathbf{W} \\cdot ([\\mathbf{a} , \\mathbf{b}]) + \\mathbf{c})\n",
    "$$\n",
    "\n",
    "Where $[,]$ is the concatenation operation and $\\mathbf{W}$ and $\\mathbf{c}$ are the weight matrix and bias of the dense layer, respectively.\n",
    "\n",
    "In this notebook, we will explore how to represent the dot product using a neural network\n",
    "\n",
    "## TODO\n",
    "1. Create a model training framework\n",
    "2. Deep vs shallow\n",
    "3. data loader to create random dot product samples\n",
    "4. overfit small sample, with high epoch count\n",
    "   1. is learning rate correct?\n",
    "\n",
    "## Setup\n",
    "```\n",
    "uv add pytorch-lightning\n",
    "uv run mlflow ui --backend-store-uri experiments\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pytorch_lightning as pyl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(pyl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int,\n",
    "        layer_sizes: List[int] = [32, 16],\n",
    "        learning_rate: float = 5e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if layer_sizes is None:\n",
    "            self.output = nn.Linear(dimension * 2, 1)\n",
    "        else:\n",
    "            layer_sizes = [dimension * 2] + layer_sizes + [1]\n",
    "            layers = []\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.output = nn.Sequential(*layers, nn.Linear(layer_sizes[-1], 1))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return self.output(torch.cat([a, b], dim=1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        a, b, y = batch\n",
    "        y_hat = self(a, b)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        a, b, y = batch\n",
    "        y_hat = self(a, b)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        a, b, y = batch\n",
    "        y_hat = self(a, b)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "We will make a dataset that creates a random pair of vectors and their dot product.\n",
    "As the data is randomly generate each batch we don't need to be concerned with overfitting.\n",
    "Therefore we have only a train dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[ 0.4740,  0.1978],\n",
      "        [-2.4661,  0.3623],\n",
      "        [ 0.3930,  0.4327],\n",
      "        [ 0.6688, -0.7077]])\n",
      "b: tensor([[ 1.1561,  0.3965],\n",
      "        [ 0.3765, -0.1808],\n",
      "        [-1.3627,  1.3564],\n",
      "        [-0.3267, -0.2788]])\n",
      "Dot Product: tensor([ 0.6265, -0.9941,  0.0513, -0.0212])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class RandomVectorDataset(Dataset):\n",
    "    def __init__(self, dimension: int, num_samples: int, seed: int = 42):\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(seed)\n",
    "        self.dimension = dimension\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a = torch.randn(self.dimension)\n",
    "        b = torch.randn(self.dimension)\n",
    "        y = torch.dot(a, b)\n",
    "        return a, b, y\n",
    "\n",
    "\n",
    "dataset = RandomVectorDataset(dimension=2, num_samples=10)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    a, b, y = batch\n",
    "    print(f\"a: {a}\")\n",
    "    print(f\"b: {b}\")\n",
    "    print(f\"Dot Product: {y}\")\n",
    "    break  # Remove this break to iterate through the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "\n",
    "def setup_experiment():\n",
    "    mlf_logger = MLFlowLogger(experiment_name=\"dot_product\", tracking_uri=\"experiments\")\n",
    "    return mlf_logger\n",
    "\n",
    "\n",
    "def train(model, dataloader):\n",
    "    early_stopping = EarlyStopping(monitor=\"train_loss\", patience=2, mode=\"min\")\n",
    "    mlf_logger = setup_experiment()\n",
    "    trainer = pyl.Trainer(\n",
    "        max_epochs=30,\n",
    "        logger=mlf_logger,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=early_stopping,\n",
    "    )\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    return trainer.test(model, dataloader)\n",
    "\n",
    "\n",
    "def train_loop(dimension: int, layers: List[int]):\n",
    "    # Create dataset and dataloader\n",
    "    num_samples = 1_000_000\n",
    "    num_samples = 1_000\n",
    "    batch_size = 1024\n",
    "    dataset = RandomVectorDataset(dimension=dimension, num_samples=num_samples)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = Model(dimension=dimension, layer_sizes=layers, learning_rate=1e-3)\n",
    "    return train(model=model, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | output | Sequential | 607    | train\n",
      "----------------------------------------------\n",
      "607       Trainable params\n",
      "0         Non-trainable params\n",
      "607       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00, 14.49it/s, v_num=d9c5, train_loss_step=2.260, train_loss_epoch=2.260]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 121.78it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch        2.4767603874206543\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# loss = train_loop(dimension=4, layers=[4, 2])\n",
    "# loss = train_loop(dimension=4, layers=[4])\n",
    "# loss = train_loop(dimension=4, layers=[16, 8, 4, 2])\n",
    "# loss = train_loop(dimension=2, layers=[4, 2])\n",
    "# loss = train_loop(dimension=2, layers=[4])\n",
    "# loss = train_loop(dimension=2, layers=[16, 8, 4, 2])\n",
    "# loss = train_loop(dimension=2, layers=[32, 32, 16, 16, 8, 8, 4, 4, 2, 2])\n",
    "loss = train_loop(dimension=2, layers=[4]*30)\n",
    "# loss = train_loop(dimension=2, layers=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all mlflow final epoch loss\n",
    "# plot against architecture and dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
